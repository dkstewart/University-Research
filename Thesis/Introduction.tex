\chapter{Introduction}
\label{chap:intro}
In our increasingly computerized world, the concept of system safety has become of great importance to many different fields. A \emph{complex safety critical system} is one whose safety cannot be shown only through testing, whose logic is difficult to comprehend without the aid of analytical tools, and that may contribute---directly or indirectly---to loss of life, damage of the environment, or large economic losses~\cite{SAE}. Critical systems can be found for example in the aviation, automotive, nuclear, and medical industries.  The process of designing such systems, from inception to deployment, presents numerous challenges with which researchers have been contending. 

System safety has been an important factor in the design of systems for many years, but the birth of system safety as we know it today began shortly after World War II. The US Air Force was having numerous aircraft accidents; over 7,700 aircraft were lost between the years of 1952 and 1966 and over 8,000 people were killed~\cite{hammer}. Their approach to aircraft system safety was to analyze the accident and ``fix" the problem for the next flight. At the time, many of the accidents were blamed on pilots, but a number of flight engineers did not believe the causes were so simple. They posited that safety must be designed and built into the aircraft~\cite{levesonWhitePaper}. With the growth of nuclear capabilities, the defense industry complex, and the overall increase of computerization, the need to abandon a ``fly-fix-fly" approach to safety was imminent~\cite{miller1954applying, levesonWhitePaper, hammer}. The goal became to avoid accidents, instead of fixing a problem after an accident occurs.

Today, system safety analysis is crucial in the development life cycle of critical systems to ensure adequate safety as well as demonstrate compliance with applicable standards. The process meant to guide the development and certification of safety critical systems has been standardized by competent authorities~\cite{SAE,SAE:ARP4761,SAE:ARP4754A}.

A prerequisite for any safety analysis is a thorough understanding of the system architecture and the behavior of its components; safety engineers use this understanding to explore the overall system behavior, assess the effect of failures on the system's safety objectives, and construct the accompanying safety analysis artifacts so that safe operation can be ensured and demonstrated~\cite{SAE:ARP4761,SAE:ARP4754A}. System information and safety artifacts can also reveal missing requirements or be used to strengthen the existing ones, and they give crucial information about how the system responds to faulty components or errors in functionality that cross component boundaries~\cite{Bozzano:2010:DSA:1951720}.  An important goal of the safety assessment process is to show what kinds of failures may occur during normal use of the system. These analyses, both qualitative and quantitative, can provide information on how the system is safe (or unsafe) for use~\cite{roland1990system}.

The development life cycle of critical systems can be roughly seen as two main thrusts that occur in tandem: one side focuses on the system development itself; the requirements of the system, the hardware and software design, and the logical behavior of the components and their interactions. The other side is safety assessment of the system. Safety analysts are concerned with the failure of a system; systems can be unsafe (fail) with or without component or software failures. Safety analysts use information generated during the system design and development process and analyze the system from the perspective of failure; in other words, they focus on what can make the system unsafe. This is used to strengthen the system design and provide feedback into the development process.

Due to the complex nature of this arrangement, these sides are in reality not always done in strict parallel and are rarely synchronized perfectly. Furthermore, the artifacts given to safety analysts from system engineers are not always formal in nature, they may come from various sources, and they often do not clearly define the entire system and its behavior. To address this concern, \emph{model-based system engineering} (MBSE) and \emph{model-based safety assessment} (MBSA) caught the attention of researchers in the safety critical system domains~\cite{Joshi05:Dasc,CAV2015:BoCiGrMa,info17:HaLuHo,5979344,Gudemann:2010:FQQ:1909626.1909813}. In model-based engineering, the development efforts are centered on a model of the intended system. Various techniques, such as formal verification, testing, test case generation, execution and animation, etc., can be used to validate and verify the proposed system behavior. Given this increase in model-based development in critical systems, leveraging the resultant models in the safety analysis process and automating the generation of safety analysis artifacts holds great promise in terms of accuracy and efficiency. 

Many of the techniques proposed for MBSA require the development of {\em fault models} specific for safety analysis; that is, the techniques do not rely on the \emph{extension} of existing system models, but rather require purpose-built fault models that are separate entities~\cite{symbAltaRica, DBLP:conf/tacas/BittnerBCCGGMMZ16, info17:HaLuHo, Gudemann:2010:FQQ:1909626.1909813}. In this approach, there is a system model used by the system engineers and a separate fault model used by safety analysts. %It requires extra manual labor to create a separate fault model that accurately describes the system model itself. 
As systems become more complex, it becomes difficult to ensure that the fault model developed for safety analysis conforms with the the model created for the development efforts -- just as it is difficult to show that the system model conforms to the actual implemented system. Another problem of this approach is that any changes made in system development are not automatically reflected in the safety analysis process; those changes must be communicated to safety analysts and incorporated into the separate fault model. This brings us right back to the problems of a non-model based approach.

Part of the safety assessment process determines how faults can manifest themselves in a particular component, but also how a manifested fault (or \emph{error}) can propagate through a system. Error propagation can be handled a variety of ways; most commonly this is done through the use of signal flow diagrams, a deep understanding of the system components, and the intuition of a good analyst~\cite{lisagor2010failure}. Various research has attempted to address this gap by providing tools that operate over a model and provide some form of propagation analysis, (e.g.,~\cite{EMV2, Joshi05:SafeComp, DBLP:conf/tacas/BittnerBCCGGMMZ16}). Other times this propagation is done explicitly (the analyst manually defines where the fault will propagate through the system)~\cite{lisagor2011model}, but as the size and complexity of industrial sized systems grow, explicit propagation can become unwieldy~\cite{Stewart17:IMBSA}. To address this problem, \emph{behavioral} propagation has been introduced~\cite{DBLP:conf/tacas/BittnerBCCGGMMZ16,stewart2020safety}. Behavioral propagation automates the process of propagating the error through the system and requires no explicit statements of what effect the error will have on components. In reality, both approaches are beneficial to an analyst. At times, there are effects that are known and easily captured explicitly. Other times, even within the same system, complex interactions make explicit propagation difficult to manage. To provide the most flexibility for an analyst, both approaches should be possible.

While using model based safety assessment, \emph{verification} of the model and its requirements can provide additional and crucial information about the system model. Verification, in this context, is the process of mathematically proving or disproving the correctness of a system with respect to certain properties or requirements. As a model and the number of system requirements grow, a scalable approach is of utmost concern. Without it, verification of the model and its requirements cannot be adequately performed. 

Commonly used artifacts in the safety assessment process are \emph{minimal cut sets}, or the minimal sets of faults that can lead to a violation of a system safety property and their associated fault trees. The automatic generation of these artifacts have been studied in depth, but have often lacked in terms of scalability\cite{minato2001zero,vesely1981fault,jung2008fast,matuzas2015dynamic,Bieber04safetyassessment}. Some research groups have introduced automating aspects of the safety assessment process and have developed tools to support this~\cite{Joshi05:SafeComp,CAV2015:BoCiGrMa,contractBasedDesign}; nevertheless, there are gaps in current capabilities we address in this dissertation. 

\section{Objectives and Summary of Contributions}
The \textbf{long range goal} of this research is to increase system safety through the support of a model-based safety assessment process backed by formal methods to help safety engineers with early detection of design issues and automation of the artifacts required for certification. The \textbf{contributions of this dissertation}, which are logical steps towards the goal, started with the definition of a modeling notation such that the information required for the safety assessment process can be easily captured in the system model. Once this notation was in place, we defined analysis procedures to verify that the system model meets its requirements in the face of failures. Further exploration of the model, component interactions, and problematic fault combinations were incorporated into these analyses in order to fully understand the safety of the system. Domain specific case studies demonstrate the feasibility of this approach.

The objectives of this dissertation were accomplished by providing the following contributions: 

\paragraph{Defined a modeling notation to capture safety information in a shared model.}
Before a fault modeling notation was defined, we chose an appropriate modeling language. The Architecture Analysis and Design Language (AADL) is an SAE International standard language that provides a unifying framework for describing the system architecture for performance-critical, embedded, real-time systems~\cite{AADL_Standard,FeilerModelBasedEngineering2012}. From its conception, AADL has been designed for the design and construction of avionics systems.  Rather than being merely descriptive, AADL models can be made specific enough to support system-level code generation; thus, results from analyses conducted, including safety analysis, correspond to the system that will be built from the model.  This specificity supports a close relationship between the system development and safety assessment processes. This modeling language was chosen for these reasons. 

We extended the AADL grammar with a safety annex extension while keeping specific fault modeling needs in mind~\cite{Stewart17:IMBSA, stewart2020safety}. The extension supports behavioral and explicit fault propagation, flexible fault modeling that allows for modeling various types of realistic component failures, and a back-end model checker that performs the analysis.  Within the AADL model, a user can add the safety annex which contains fault definitions for components. The flexibility of the fault definitions allows for either complex or simple fault behavior. This allows analysts to capture realistic faulty components and scenarios in the model. When a fault is activated, it modifies the output of the component. This faulty behavior may lead to a violation of the contracts of other components in the system, including assumptions of downstream components. The model checker analyzes the impact of a fault when the safety analysis is executed on the extended model.

\paragraph{Defined analysis procedures to verify behavior of the model in the presence of faults.}
Given a safety property or requirement, it is useful to see if the property can be verified when faults are present (or active) in the system model. The fault analysis statement---also referred to as the fault hypothesis---resides in the AADL system implementation that is selected for verification. The hypothesis statement may specify either a maximum number of faults that can be active at any point in execution (\emph{max n fault hypothesis}) or that the only faults to be considered are those whose probability of simultaneous occurrence is above some probability threshold (\emph{probabilistic hypothesis}).  In the former case, we assert that the number of simultaneous faults is at or below some integer threshold.  In the latter, we determine all combinations of faults whose probabilities are above the specified probability threshold, and describe this as a proposition over the fault variables in the model. If any combination of faults is within allowable parameters during analysis and this causes a property violation, the user can view the state history of the system. This analysis provides valuable system information about the relationship between the requirement of interest and the defined faults in the model~\cite{stewart2020safety,stewart2021safety}. 

\paragraph{Provided in depth analysis capabilities that explore system models and compositionally derive sets of fault trees and associated minimal cut sets.}
The minimal sets of faults that when active can violate a safety property, or {\em minimal cut sets}, are commonly used in the assessment and certification of critical systems. Since the introduction of cut sets in the field of safety analysis, much research has been performed to address their generation~\cite{fta:survey,rauzy1993new,historyFTA,Bozzano:2010:DSA:1951720,rausand2003system}. One of the ongoing problems with minimal cut set generation is the inability to scale to industrial-sized systems. As the system gets larger, more minimal cut sets are possible with ever increasing cardinality. In recent years, researchers have leveraged model checking to address this problem.~\cite{bieber2002combination,schafer2003combining,fta:survey,contractBasedDesign,symbFTA,DBLP:conf/cav/BozzanoCPJKPRT15}. We have pushed forward on this front and found a way to generate these sets in a \emph{compositional} fashion by composing sets of fault trees. Compositional verification performs the proof in a per-architectural-layer approach; this divides a very large proof over the entire system into smaller proofs over each layer of the system. These smaller proofs are then composed together to provide the system level proof. 

The composition of {\em fault forests} (sets of fault trees) is also performed per layer. We extend the underlying transition system with Boolean literals that correspond with fault activations. This information is used to determine which active faults may lead to a violation of a safety property. The Boolean formula that corresponds with these potentially active faults and violated safety property is composed per layer of the architecture.

To our knowledge, composition of fault forests has not been previously performed. This research formalizes the composition of fault forests and implements the associated algorithm in tools supporting the safety annex.

\paragraph{Explored how the formal specification of requirements can change analysis results.}
Splitting a complex requirement into its constituent conjuncts introduces the possibility of changing certain analysis results. Because the compositional analysis of fault forests relies on the requirements for each component, it is natural to question how the structure of the requirements may affect analysis results. We explored this idea by automatically decomposing requirements into smaller subexpressions and rewriting them into semantically equivalent but syntactically (structurally) different forms, and compared analysis results between the original contracts and the rewritten contracts and discussed the findings. We found that as the requirements became more \emph{granular}, i.e., split into more conjuncts~\cite{ghassabani_2018}, the proof cores computed per layer show which subformulae of an equation were necessary to prove a safety property. The specificity of requirements we refer as \emph{granularity} and this idea ties into a broader discussion of the ideas underlying requirement engineering, behavioral modeling, minimal cut sets, and system development. 

\paragraph{Demonstrated the objectives of this proposal by use of case studies.}
A case study from the safety critical aerospace domain illustrates the process of using the safety annex for AADL and demonstrates the capabilities of the implemented analyses described in this research. We perform various timing experiments to provide insight into the scalability of the approach. Furthermore, numerous subsystem examples are given throughout the dissertation to illustrate specific capabilities and solutions. These examples demonstrate how the safety analysis process described in here can be applied in the domain of aerospace and other critical systems domains. \\

In summary, this dissertation provides a modeling notation that supports a close relationship between the system development and safety assessment processes, and it defines analysis procedures that verify that the system model meets requirements in the presence of faults. This research also provides a formalism that defines compositional derivation of fault forests, and it explores the granularity of contracts and how that affects analysis results. Finally, the demonstration of these contributions are shown through use of case studies. 

\section{Structure of this Document}
This dissertation is organized into 8 chapters. Chapter~\ref{chap:prelim} discusses the preliminaries, related work, and an overview of formal verification. Chapter \ref{chap:faultModeling} provides a detailed look at fault modeling in complex critical systems, and the safety annex and its implementation. Chapter~\ref{chap:compFF} describes the composition of fault forests and provides the formalisms and algorithms; this is followed by a chapter on case studies. Chapter \ref{chap:granularity} provides the initial exploration of how contract specification can change the results of the analysis. We include Chapter~\ref{ch:discussion} as a discussion of this research and how it could be extended. Lastly, the conclusion in Chapter~\ref{chap:conclusion} summarizes the dissertation.











