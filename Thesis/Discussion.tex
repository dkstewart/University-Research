\chapter{Discussions and Future Work}
\label{ch:discussion}
The overarching goal of this dissertation is to utilize the capability of a model checker to provide information that can be used during the safety analysis process. We used a model checker to provide behavioral propagation of errors throughout nominal model contract and extended the transition system in order to formalize the composition of fault forests. In the implementation of the formalism, we leveraged MIVC generation to provide compositional minimal cut set generation, and we used counterexample generation capability to gain insight into the state of a system when a property is violated. All of this can be used within the iterative process of complex system design and development to show safety of a system and to drive for design change (see Section~\ref{subsec:mbsa}). We discuss in this chapter other ways of providing the analyst with safety critical information regarding model elements that may be of interest. 

\section{Preprocessing Improvements}
\label{sec:preproc}
The preprocessing that must be done to generate minimal cut sets is not trivial (see discussion in Section~\ref{sec:impl}). All minimal inductive validity cores must be produced which is as hard as model checking~\cite{GhassabaniGW16}. If all MIVCs are found, then these sets must be transformed into minimal correction sets through a minimal hitting set algorithm. Finding the hitting sets, or {\em minimal set cover} is an NP-hard problem~\cite{gainer2017minimal,karp1972reducibility}.%, but irreducibility is a weaker requirement and thus can be performed in polynomial time~\cite{liffiton2005finding}. While this is no longer NP-hard, the possible number of MIVCs may be large and thus the computation impractical. 
We performed extensive research into hitting set algorithms and implemented an open source option that performed well on benchmark testing~\cite{murakami2013efficient}, but nevertheless, these preprocessing steps are intractable. We believe that a more direct approach to computing the minimal correction sets (MCSs) could be beneficial and may be possible to implement as an additional JKind engine. This would eliminate the need to compute MIVCs and avoid the hitting set algorithm altogether. 

Liffiton et al.~\cite{liffiton2016fast} considered the enumeration of MCSs and MUSs as an exploration of the power set of all subclauses in a formula. The subsets form a lattice through subset relations and this lattice can be explored in clever ways in order to enumerate these related sets. A \texttt{map} is a boolean function used to encode the explored portions of the lattice. The entire lattice can be split into two regions based on the feasibility of the subset: these are the {\em maximally satisfiable subsets} (MSS) and the {\em minimally unsatisfiable subsets} (MUS). A maximally satisfiable subset is the complement of a MCS with respect to the constraint system; more formally, an MSS $M$ of a constraint system $C$ is a subset $M\subseteq C$ such that $M$ is satisfiable and $\forall c \in C \setminus M$ : $M \cup \{c\}$ is unsatisfiable. A full enumeration of the MSSs (also called the Max-SAT problem) will easily provide the full enumeration of the MCSs by taking the complement in $C$. 

The lattice structure ensures that any subset of a maximally satisfiable set is still satisfiable; thus, as subsets are explored, large portions of the map are eliminated from consideration. The CAMUS algorithm presented by Liffiton et al.~\cite{liffiton2005finding} uses the hitting set duality between MCS and MUS to produce MUSs by first computing MSSs, finding the complements (MCSs), and then producing the hitting sets (MUSs). It computes MCSs with what can be considered a top-down search through the power set, searching a level (subsets of a particular size) for satisfiable subsets that are not subsumed by some larger satisfiable subset found in a higher level. Whenever an MSS is found, all subsets are blocked from the \texttt{map} and the search moves on to the next level of the power set. The algorithms presented for this SAT-solver based enumeration of MUSs are (1) find all MCSs, and (2) compute MUSs. Related research has recently expanded upon the former algorithm used to enumerate MCSs by employing a correction set rotation method and simultaneously implementing the strengthening and relaxing methods described in the CAMUS algorithm~\cite{narodytska2018core}.  

We believe that these algorithms can be adjusted for SMT solvers and infinite state transition systems appropriately and then implemented in JKind. This will allow for a direct computation of MCSs and avoid the two preprocessing steps (MIVC generation and MHS generation) as outlined in this dissertation. 



\section{Graphical Fault Trees}
A typical fault tree generated by many of the current research tools available shows very little hierarchical information. An example of a flat fault tree is shown in Figure~\ref{fig:flatFT} and shows only the minimal cut sets that contribute to the top level event (TLE). 
\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/flatFT.pdf}
	\caption{Flat Fault Tree for a Sensor Example}
	\label{fig:flatFT}
\end{figure}

The result of computing the minimal cut sets and presenting them in a tree-like structure is exactly this: a very short tree (one layer) with many branches. In essence, this provides no further information than that of a graphical representation of the minimal cut sets. The nature of our approach is that the fault forest generation is performed compositionally; this gives information regarding each level of analysis per each layer of architecture. This information can be displayed in a fault tree structure and can describe the relationship between an active fault and the supporting guarantees it violates. An example of a hierarchical fault tree that could be produced by the information gathered through the minimal cut set algorithms defined in Chapter~\ref{chap:compFF} is shown in Figure~\ref{fig:ftSensor}. 

\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/ftSensor.pdf}
	\caption{Hierarchical Fault Tree for a Sensor Example}
	\label{fig:ftSensor}
\end{figure}

Each layer of the architecture is shown in the fault tree and represented by the violated contracts of that layer. The OR-gate between the lower level contracts and the top level event (violation of the safety property) reflects the property nicely: if either of the subsystems fail, the safety property is violated. The second layer of the tree also reflects the relationship between the sensors and the sensor subsystem: if any pairwise combination of sensors fail, the mid-level contract is violated. This process of using MIVCs to generate minimal cut sets will provide layer by layer the necessary information to not only collect the minimal cut sets, but also reflect the hierarchical nature of the system within the fault trees generated.

Upon initial exploration of the idea, two problems became apparent: (1) this relies on architectural depth of the model and provides no additional information if the architecture is a single layer, and (2) safety analysts wish to see fault trees as a kind of signal flow diagram and require a trace of the signal flow from the error to the property violation. While our initial idea could partially address these issues, we began to look into a different method of analysis that may be able to address both simultaneously. 

It became quickly apparent that to include in the analysis results information regarding the roles that faults, components, and node inputs play within a proof would provide great feedback to an analyst. They could view minimal cut sets from the perspective of the entire model and not just the faults that are active. If, for instance, certain components play direct roles in the proof of a safety property, those components would be seen as critical and managed or analyzed in a more comprehensive way. Likewise if specific inputs were known to be crucial to a proof, they would also be critical within the safety analysis. Preliminary investigations showed that related work may be used and extended to perform this kind of analysis~\cite{9081652}. We discuss this in the next two subsections.

\input{granularityMutationEq}
\input{granularityMutationInputs}


\section{Compositional Probabilistic Analysis}
Safety analysis techniques aim at demonstrating that the system meets the requirements necessary for certification and use in the presence of faults. In many domains, there are two main steps to this process: (1) the generation of all minimal cut sets, and (2) the computation of the corresponding fault probability, i.e., the probability of violating the safety property given probabilities for all faults in the system. 

The probability of the Top Level Event (TLE), or violation of the safety property, is used to find the likelihood of the safety hazard that it represents. While evaluation of the fault model with a given probabilistic threshold does provide information on the safety hazards, it is also informative to know the overall probability of the occurrence of a hazard. 

Such computations can be carried out by leveraging the logical formula represented by the disjunction of all minimal cut sets which are in turn conjunctions of their constituents. 

Given a set of minimal cut sets and a mapping $\mathcal{P}$ that gives the probability of the basic faults in the system $f_i$, it is possible to compute the probability of occurrence of the TLE. Assuming that the basic faults are independent, the probability of a single minimal cut set, $\sigma$ is given by the product of the probabilities of its basic faults:
\begin{center}
    \begin{equation*}\mathcal{P}(\sigma) = \prod_{f_i \in \sigma} \mathcal{P}(f_i) 
    \end{equation*}    
\end{center}

For a set of minimal cut set, $S$, the probability can be computed using the following recursive formula:

\begin{center}
    $\mathcal{P}(S_1 \cup S_2) = \mathcal{P}(S_1) + \mathcal{P}(S_2) - \mathcal{P}(S_1 \cap S_2)$
\end{center}

Given the independence assumption, $\mathcal{P}(S_1 \cap S_2)$ is computed as $\mathcal{P}(S_1) \cdot   \mathcal{P}(S_2)$. Using this technique, it is theoretically possible to compute the overall probability of a TLE given all minimal cut sets and an independence assumption, but in the real world of safety analysis this poses some problems, the largest of which is scalability. Given a very large system with many possible faults, it becomes difficult to compute all minimal cut sets without pruning of any kind. If one is unable to complete such computations, it is not possible to simply compute the probabilities as described above. 

As previously discussed, it is standard practice to consider cut sets only up to a given cardinality. As the cardinality of the cut sets increase the likelihood of their occurrence decreases. As the system increases in size the possible combinations of problematic faults will inevitably increase -- at times exponentially. In order to simplify these computations and address scalability, minimal cut sets up to a certain cardinality are considered. Everything above that is ``safely" ignored, and then specific criteria is used to over-approximate the error. The end result of these computations is above the actual probability (i.e., a safe approximation), but close enough to be significant. 

Another drawback to computing an exact probability of the TLE is the problem of model reliability and exactness; this corresponds exactly to the issue of granularity discussed in Chapter~\ref{chap:granularity}. For instance, if two groups of engineers each built a model of the same system, the models may not be equivalent in terms of behavioral properties. Since our approach of computing minimal cut sets depends on the properties over system components, the calculated top-level probability will change. Different representations of the system will alter the computations.  

As an example, assume that Figure~\ref{fig:probComp1} is a snapshot of a given layer in a system designed by the first group of engineers. Component A has a contract, $G_A$, which is determined by the \aivcalg algorithm to depend on a lower level contract, $g_A = a \land b$. $g_A$ will be in the set of \textit{IVC}s for the contract $G_A$. Assume that only $a$ is required for the proof of $G_A$. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=6cm]{images/probComp1.PNG}
\caption{Sample System Contract Part I} \label{fig:probComp1}
\end{center}
\end{figure}

Two faults are defined on component B: $f_1$ violates $a$ and $f_2$ violates $b$. Since each of these faults will violate the contract $g_A$, each of these faults will be found in minimal cut set for $G_A$.

Now assume that Figure~\ref{fig:probComp2} was the system representation built by the second group of engineers. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=6cm]{images/probComp2.PNG}
\caption{Sample System Contract Part II} \label{fig:probComp2}
\end{center}
\end{figure} 
The basic system structure is the same, but this time there are two contracts for component B: $g_1 = a$ and $g_2 = b$. Since $b$ is not required for the proof of $G_A$, only $g_1$ is found in the \textit{IVC}s for $G_A$ and thus only $f_1$ will be seen in the minimal cut sets for this particular contract. 

In this simple example, it is easy to see why a single computation of the top-level probability of a system may be misleading and may not reflect the actual probability of the system. To this end, we wish to find a way to accurately obtain higher and lower bounds of what the probability is likely to be. 

A lower bound for the probability can be obtained by choosing a maximum cardinality for each minimal cut set before computations begin, e.g. assume that cardinalities above $n$ are too unlikely to be significant. This will contribute to better scalability in large systems with numerous possible cut sets. A higher bound is commonly assigned by experts of the system, e.g. around 3 orders of magnitude higher than the lower approximation. 

It would be beneficial to leverage the  MIVC information to provide both the lower and upper bound approximations and show that these values are significant and trustworthy. 




%We also believe that the total verification time can be improved by focusing on preprocessing steps. In the minimal cut set generation preprocessing, the MIVCs are collected, the hitting sets computed, and then our algorithm is run based on the minimal correction sets found in the previous two steps. By eliminating the need for MIVCs altogether and directly computing the minimal correction sets from the Lustre program, we believe this could improve computation time. 
