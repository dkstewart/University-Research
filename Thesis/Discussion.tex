\chapter{Discussions and Future Work}
\label{ch:discussion}
The overarching goal of this dissertation is to utilize the capability of a model checker to provide information that can be used during the safety analysis process. We leveraged the model checker to provide behavioral propagation of errors throughout nominal model contracts, we leveraged MIVC generation to provide compositional minimal cut set generation, and we use the counterexample generation capability to gain insight into the state of a system when a property is violated. All of this can be used within the iterative process of complex system design and development to show safety of a system and to drive for design change (see Section~\ref{subsec:mbsa}). We discuss in this chapter other ways of providing the analyst with safety critical information regarding model elements that may be of interest. 

\section{Graphical Fault Trees}
A typical fault tree generated by many of the current research tools available shows very little hierarchical information. An example of a flat fault tree is shown in Figure~\ref{fig:flatFT} and shows only the minimal cut sets that contribute to the top level event (TLE). 
\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/flatFT.pdf}
	\caption{Flat Fault Tree for a Sensor Example}
	\label{fig:flatFT}
\end{figure}

The result of computing the minimal cut sets and presenting them in a tree-like structure was similar to this: a very short tree (one layer) with many branches. In essence, this provides no further information than that of a textual representation of the minimal cut sets. The nature of our minimal cut set generation approach is that the proofs are performed compositionally; this gives information regarding each level of analysis {\em per} each layer of architecture. This information can be used and printed out in a fault tree-like structure and, depending on the model, may provide the links between an active fault and the supporting guarantees it violates. An example of a hierarchical fault tree that could be produced by the information gathered through the minimal cut set algorithms defined in Chapter~\ref{chap:mcsGen} is shown in Figure~\ref{fig:ftSensor}. 

\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/ftSensor.pdf}
	\caption{Hierarchical Fault Tree for a Sensor Example}
	\label{fig:ftSensor}
\end{figure}

Each layer of the architecture is shown in the fault tree and represented by the violated contracts of that layer. The OR-gate between the lower level contracts and the top level event (violation of the safety property) reflects the property nicely: if either of the subsystems fail, the safety property is violated. The second layer of the tree also reflects the relationship between the sensors and the sensor subsystem: if any pairwise combination of sensors fail, the mid-level contract is violated. This process of using MIVCs to generate minimal cut sets will provide layer by layer the necessary information to not only collect the minimal cut sets, but also reflect the hierarchical nature of the system within the fault trees generated.

Upon initial exploration of the idea, two problems became apparent: (1) this relies on architectural depth of the model and provides no additional information if the architecture is a single layer, and (2) safety analysts wish to see fault trees as a kind of signal flow diagram and require a trace in signal from the error to the property violation. While our initial idea could partially address both issues, we began to look into a different method of analysis that may be able to address both issues. 

It became quickly apparent that to include in the analysis results information regarding the roles that faults, components, and node inputs play within a proof would provide great feedback to an analyst. They could view minimal cut sets from the perspective of the entire model and not just the faults that are active. If, for instance, certain components play direct roles in the proof of a safety property, those components would be seen as critical and managed or analyzed in a more comprehensive way. Likewise if specific inputs were known to be crucial to a proof, they would also be critical within the safety analysis. Preliminary investigations showed that related work may be used and extended to perform this kind of analysis~\cite{9081652}. 

\section{Compositional Probabilistic Analysis}
Safety analysis techniques aim at demonstrating that the system meets the requirements necessary for certification and use in the presence of faults. In many domains, there are two main steps to this process: (1) the generation of all minimal cut sets, and (2) the computation of the corresponding fault probability, i.e., the probability of violating the safety property given probabilities for all faults in the system. 

The probability of the Top Level Event (TLE), or violation of the safety property, is used to find the likelihood of the safety hazard that it represents. While evaluation of the fault model with a given probabilistic threshold does provide information on the safety hazards, it is also informative and desirable to find the overall probability of the occurrence of a hazard. 

Such computations can be carried out by leveraging the logical formula represented by the disjunction of all minimal cut sets which are in turn conjunctions of their constituents. 

Given a set of minimal cut sets and a mapping $\mathcal{P}$ that gives the probability of the basic faults in the system $f_i$, it is possible to compute the probability of occurrence of the TLE. Assuming that the basic faults are independent, the probability of a single minimal cut set, $\sigma$ is given by the product of the probabilities of its basic faults:
\begin{center}
    \begin{equation*}\mathcal{P}(\sigma) = \prod_{f_i \in \sigma} \mathcal{P}(f_i) 
    \end{equation*}    
\end{center}

For a set of minimal cut set, $S$, the probability can be computed using the following recursive formula:

\begin{center}
    $\mathcal{P}(S_1 \cup S_2) = \mathcal{P}(S_1) + \mathcal{P}(S_2) - \mathcal{P}(S_1 \cap S_2)$
\end{center}

Due to the independence assumption, $\mathcal{P}(S_1 \cap S_2)$ is computed as $\mathcal{P}(S_1) \cdot   \mathcal{P}(S_2)$. Using this technique, it is theoretically possible to compute the overall probability of a TLE given all minimal cut sets and an independence assumption, but in the real world of safety analysis this poses some problems, the largest of which is scalability. Given a very large system with many possible faults, it becomes difficult to compute all minimal cut sets without pruning of any kind. If one is unable to complete such computations, it is not possible to simply compute the probabilities as described above. 

As previously discussed, it is standard practice to consider cut sets only up to a given cardinality. As the cardinality of the cut sets increase, the likelihood of their occurrence decreases and as the system increases in size, the possible combinations of problematic faults will inevitably increase, at times exponentially. In order to simplify these calculations and address the problem of scalability, minimal cut sets up to a certain cardinality are considered. Everything above that is ``safely" ignored, and then specific criteria is used to over-approximate the error. The end result of these computations is above the actual probability (i.e., a safe approximation), but close enough to be significant. 

Another drawback to computing an exact probability of the TLE is the problem of model reliability and exactness; this corresponds exactly to the issue of granularity discussed in Chapter~\ref{chap:granularity}. For instance, if two groups of engineers each built a model of the same system, the models may not be equivalent; especially in terms of behavioral properties. Since our approach of computing minimal cut sets depends on the properties over system components, the calculated top-level probability will change. Different representations of the system will alter the computations.  

As an example, assume that Figure~\ref{fig:probComp1} is a snapshot of a given layer in a system designed by the first group of engineers. Component A has a contract, $G_A$, which is determined by the \aivcalg algorithm to depend on a lower level contract, $g_A = a \land b$. $g_A$ will be in the set of \textit{IVC}s for the contract $G_A$. Assume that only $a$ is required for the proof of $G_A$. 

\begin{figure}[h]
\begin{center}
\includegraphics[width=6cm]{images/probComp1.PNG}
\caption{Sample System Contract Part I} \label{fig:probComp1}
\end{center}
\end{figure}

Two faults are defined on component B: $f_1$ violates $a$ and $f_2$ violates $b$. Since each of these faults will violate the contract $g_A$, each of these faults will be found in minimal cut set for $G_A$.

Now assume that Figure~\ref{fig:probComp2} was the system representation built by the second group of engineers. 
\begin{figure}[h]
\begin{center}
\includegraphics[width=6cm]{images/probComp2.PNG}
\caption{Sample System Contract Part II} \label{fig:probComp2}
\end{center}
\end{figure} 
The basic system structure is the same, but this time there are two contracts for component B: $g_1 = a$ and $g_2 = b$. Since $b$ is not required for the proof of $G_A$, only $g_1$ is found in the \textit{IVC}s for $G_A$ and thus only $f_1$ will be seen in the minimal cut sets for this particular contract. 

In this simple example, it is easy to see why a single computation of the top-level probability of a system may be misleading and may not reflect the actual probability of the system. To this end, we wish to find a way to accurately obtain higher and lower bounds of what the probability is likely to be. 

A lower bound for the probability can be obtained by choosing a maximum cardinality for each minimal cut set before computations begin, e.g. assume that cardinalities above $n$ are too unlikely to be significant. This will contribute to better scalability in large systems with numerous possible cut sets. A higher bound is commonly assigned by experts of the system, e.g. around 3 orders of magnitude higher than the lower approximation. 

It would be beneficial to leverage the  MIVC information to provide both the lower and upper bound approximations and show that these values are significant and trustworthy. 

\include{granularityMutationEq}
\include{granularityMutationInputs}

\section{Preprocessing Improvements}
We also believe that the total verification time can be improved by focusing on preprocessing steps. In the minimal cut set generation preprocessing, the MIVCs are collected, the hitting sets computed, and then our algorithm is run based on the minimal correction sets found in the previous two steps. By eliminating the need for MIVCs altogether and directly computing the minimal correction sets from the Lustre program, we believe this could improve computation time. 
