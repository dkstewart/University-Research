\chapter{Discussions}
\label{ch:discussion}
The overarching goal of this dissertation is to utilize the capability of a model checker to provide information that can be used during the safety analysis process. We used a model checker to provide behavioral propagation of errors throughout nominal model contract and extended the transition system in order to formalize the composition of fault forests. In the implementation of the formalism, we leveraged MIVC generation to provide compositional minimal cut set generation, and we used counterexample generation capability to gain insight into the state of a system when a property is violated. All of this can be used within the iterative process of complex system design and development to show safety of a system and to drive for design change (see Section~\ref{subsec:mbsa}). We discuss in this chapter other ways of providing the analyst with safety critical information regarding model elements that may be of interest. 


\section{Preprocessing Improvements}
\label{sec:preproc}
The preprocessing that must be done to generate minimal cut sets is not trivial (see discussion in Section~\ref{sec:impl}). All minimal inductive validity cores must be produced which is as hard as model checking~\cite{GhassabaniGW16}. If all MIVCs are found, then these sets must be transformed into minimal correction sets through a minimal hitting set algorithm. Finding the hitting sets, or {\em minimal set cover} is an NP-hard problem~\cite{gainer2017minimal,karp1972reducibility}.%, but irreducibility is a weaker requirement and thus can be performed in polynomial time~\cite{liffiton2005finding}. While this is no longer NP-hard, the possible number of MIVCs may be large and thus the computation impractical. 
We performed extensive research into hitting set algorithms and implemented an open source option that performed well on benchmark testing~\cite{murakami2013efficient}, but nevertheless, these preprocessing steps are intractable. We believe that a more direct approach to computing the minimal correction sets (MCSs) could be beneficial and may be possible to implement as an additional JKind engine. This would eliminate the need to compute MIVCs and avoid the hitting set algorithm altogether. 

Liffiton et al.~\cite{liffiton2016fast} considered the enumeration of MCSs and MUSs as an exploration of the power set of all subclauses in a formula. The subsets form a lattice through subset relations and this lattice can be explored in clever ways in order to enumerate these related sets. A \texttt{map} is a boolean function used to encode the explored portions of the lattice. The entire lattice can be split into two regions based on the feasibility of the subset: these are the {\em maximally satisfiable subsets} (MSS) and the {\em minimally unsatisfiable subsets} (MUS). A maximally satisfiable subset is the complement of a MCS with respect to the constraint system; more formally, an MSS $M$ of a constraint system $C$ is a subset $M\subseteq C$ such that $M$ is satisfiable and $\forall c \in C \setminus M$ : $M \cup \{c\}$ is unsatisfiable. A full enumeration of the MSSs (also called the Max-SAT problem) will easily provide the full enumeration of the MCSs by taking the complement in $C$. 

The lattice structure ensures that any subset of a maximally satisfiable set is still satisfiable; thus, as subsets are explored, large portions of the map are eliminated from consideration. The CAMUS algorithm presented by Liffiton et al.~\cite{liffiton2005finding} uses the hitting set duality between MCS and MUS to produce MUSs by first computing MSSs, finding the complements (MCSs), and then producing the hitting sets (MUSs). It computes MCSs with what can be considered a top-down search through the power set, searching a level (subsets of a particular size) for satisfiable subsets that are not subsumed by some larger satisfiable subset found in a higher level. Whenever an MSS is found, all subsets are blocked from the \texttt{map} and the search moves on to the next level of the power set. The algorithms presented for this SAT-solver based enumeration of MUSs are (1) find all MCSs, and (2) compute MUSs. Related research has recently expanded upon the former algorithm used to enumerate MCSs by employing a correction set rotation method and simultaneously implementing the strengthening and relaxing methods described in the CAMUS algorithm~\cite{narodytska2018core}.  

We believe that these algorithms can be adjusted for SMT solvers and infinite state transition systems appropriately and then implemented in JKind. This will allow for a direct computation of MCSs and avoid the two preprocessing steps (MIVC generation and MHS generation) as outlined in this dissertation. 



\section{Graphical Fault Trees}
A typical fault tree generated by many of the current research tools available shows very little hierarchical information. and tends to show only the minimal cut sets that contribute to the top level event (TLE). 
\begin{comment}
\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/flatFT.pdf}
	\caption{Flat Fault Tree for a Sensor Example}
	\label{fig:flatFT}
\end{figure}
\end{comment}

The result of computing the minimal cut sets and presenting them in a tree-like structure is a one layer tree with many branches. The nature of our approach is that the fault forest generation is performed compositionally; this gives information regarding each level of analysis per layer of architecture. This information can be displayed in a fault tree structure and can describe the relationship between an active fault and the supporting guarantees it violates at any layer of the system architecture. An example of a hierarchical fault tree is shown in Figure~\ref{fig:ftSensor}. 

\begin{figure}[h!]
	\centering
	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{images/ftSensor.pdf}
	\caption{Hierarchical Fault Tree for a Sensor Example}
	\label{fig:ftSensor}
\end{figure}

%Each layer of the architecture is shown in the fault tree and represented by the violated contracts of that layer. The OR-gate between the lower level contracts and the top level event (violation of the safety property) reflects the property nicely: if either of the subsystems fail, the safety property is violated. The second layer of the tree also reflects the relationship between the sensors and the sensor subsystem: if any pairwise combination of sensors fail, the mid-level contract is violated. 
This process of using MIVCs to generate minimal cut sets provides layer by layer the necessary information to not only collect the minimal cut sets, but also reflect the hierarchical nature of the system within the fault trees generated. The output of the safety annex is textual and can include hierarchical information, but is not in graphical form. This is a possible extension to the work provided in this dissertation. 

%Upon initial exploration of the idea, two problems became apparent: (1) this relies on architectural depth of the model and provides no additional information if the architecture is a single layer, and (2) safety analysts wish to see fault trees as a kind of signal flow diagram and require a trace of the signal flow from the error to the property violation. While our initial idea could partially address these issues, we began to look into a different method of analysis that may be able to address both simultaneously. 


%It became quickly apparent that to include in the analysis results information regarding the roles that faults, components, and node inputs play within a proof would provide great feedback to an analyst. 

%They could view minimal cut sets from the perspective of the entire model and not just the faults that are active. If, for instance, certain components play direct roles in the proof of a safety property, those components would be seen as critical and managed or analyzed in a more comprehensive way. Likewise if specific inputs were known to be crucial to a proof, they would also be critical within the safety analysis. Preliminary investigations showed that related work may be used and extended to perform this kind of analysis~\cite{9081652}. We discuss this in the next two subsections.

%\input{granularityMutationEq}
%\input{granularityMutationInputs}


\section{Compositional Probabilistic Analysis}
Safety analysis techniques aim at demonstrating that the system meets the requirements necessary for certification and use in the presence of faults. In many domains, there are two main steps to this process: (1) the generation of all minimal cut sets, and (2) the computation of the corresponding fault probability, i.e., the probability of violating the safety property given probabilities for all faults in the system. 

The probability of the Top Level Event (TLE), or violation of the safety property, is used to find the likelihood of the safety hazard that it represents. While evaluation of the fault model with a given probabilistic threshold does provide information on the safety hazards, it is also informative to know the overall probability of the occurrence of a hazard. Such computations can be carried out by leveraging the logical formula represented by the disjunction of all minimal cut sets which are in turn conjunctions of their constituents. 

%Given a set of minimal cut sets, it is entirely possible to compute the probability of occurrence of the TLE. In the world of safety analysis this poses some problems, the largest of which is scalability. Given a very large system with many possible faults, it becomes difficult to compute all minimal cut sets without pruning of any kind. 
It is standard practice to consider cut sets only up to a given cardinality. As the cardinality of the cut sets increase the likelihood of their occurrence decreases. As the system increases in size the possible combinations of problematic faults will inevitably increase. In order to simplify minimal cut set generation, sets only up to a certain cardinality are considered. Everything above that is ``safely" ignored, and then specific criteria is used to over-approximate the error. The end result of these computations is above the actual probability (i.e., a safe approximation), but close enough to be useful. 

%Another difficulty in computing an exact probability of the TLE corresponds to the issue of granularity discussed in Chapter~\ref{chap:granularity}. For instance, if two groups of engineers each built a model of the same system, the models may not be equivalent in terms of behavioral properties. Since our approach of computing minimal cut sets depends on the properties over system components, the calculated top-level probability may change. Different representations of the system may alter the computations.  
%A lower bound for the probability can be obtained by choosing a maximum cardinality for each minimal cut set before computations begin, e.g. assume that cardinalities above $n$ are too unlikely to be significant. This will contribute to better scalability in large systems with numerous possible cut sets. A higher bound is commonly assigned by experts of the system, e.g. around 3 orders of magnitude higher than the lower approximation. 

It would be beneficial to leverage the MIVC information to provide both the lower and upper bound approximations and show that these values are significant and trustworthy. 




%We also believe that the total verification time can be improved by focusing on preprocessing steps. In the minimal cut set generation preprocessing, the MIVCs are collected, the hitting sets computed, and then our algorithm is run based on the minimal correction sets found in the previous two steps. By eliminating the need for MIVCs altogether and directly computing the minimal correction sets from the Lustre program, we believe this could improve computation time. 
